# **基于AlphaGo Zero的动态任务调度系统**

## 1. 体系架构总览

本系统旨在构建一个先进的强化学习智能体，通过将有向无环图（DAG）任务调度问题建模为一个确定性的、单玩家的游戏，以求解决其核心优化目标——最小化**最终完工时间（Makespan）**。

该体系架构由五个深度耦合、相互协作的核心组件构成，并在一个分布式的自对弈（Self-Play）强化学习框架内进行迭代优化，形成一个完整的、闭环的、自我进化的生态系统：

1.  **动态问题生成器 (`dag_generator.py`)**: 作为训练课程的源头，负责程序化地生成无穷无尽、具备高度多样性的调度问题实例。
2.  **调度环境 (`environment.py`)**: 精心打造的“游戏世界”，负责精确模拟调度过程、管理状态、执行动作并计算最终结果。
3.  **双头神经网络 (`model.py`)**: 系统的“认知核心”，以强大的**图Transformer (Graph Transformer)**为骨干网络，为任何给定的调度状态提供策略与价值的先验估计。
4.  **蒙特卡洛树搜索 (`mcts.py`)**: 系统的“规划引擎”，它利用神经网络的战略直觉进行深度前瞻性搜索，将宏观判断提炼为战术上更优的决策。
5.  **训练管线 (`main.py`, `trainer.py`)**: 一个集成了所有组件的、复杂的分布式系统。它包含了**专家知识预训练、自对弈、模型训练、竞技场评估与模型晋升**等关键环节，驱动智能体能力实现螺旋式的持续提升。

---

## 2. 组件一：动态问题生成器 (`dag_generator.py`) —— 无偏见的课程引擎

这是整个学习系统的起点，负责为智能体提供一个难度可控、永不枯竭的训练课程，是其泛化能力的基石。

*   **角色与定位**: 在每一局自对弈游戏开始前，`ProblemGenerator`被调用，以生成一个全新的、随机的`SchedulingProblem`实例。

*   **核心生成流程**:
    1.  **参数随机化**: 根据预设的参数**范围**（如`num_tasks_range`, `density_range`, `num_processors_range`等），为即将生成的问题实例随机选择一组具体的参数。这确保了每一局“游戏”的规模、依赖复杂度、资源限制都可能不同。
    2.  **无偏见DAG生成**:
        *   **候选边池的构建与完全随机化**: 算法从一个包含`N`个孤立节点的空图开始。首先，创建一个包含所有`N * (N-1)`条可能的有向边的“候选池”。随后，**将该池完全随机打乱**。这是确保生成过程无任何结构性偏见、能够理论上覆盖所有DAG拓扑的关键步骤。
        *   **迭代加边与周期检测**: 算法按随机顺序遍历候选池。对于每一条候选边 `u -> v`，会执行一个高效的**周期检测**：检查当前图中是否已存在一条从 `v` 到 `u` 的路径。若不存在，则采纳该边；反之，则丢弃，以保证图的无环特性。
        *   **基于密度的边数控制**: 加边过程会持续进行，直到图中成功添加的边数达到一个由随机`density`参数计算出的目标数量为止。
    3.  **结构标准化 (传递性约简)**: 对生成的图执行严格的**传递性约简**算法。该步骤移除了图中的冗余依赖边（例如，若存在A->B->C的路径，则直接的A->C边是冗余的），确保输入给神经网络的图结构是其所代表的逻辑依赖关系的最简、最本质的表示。
    4.  **成本与环境分配**: 使用NumPy的向量化操作，高效地为所有任务节点和依赖边从预设的范围中随机分配计算与通信成本。同时，随机生成处理器数量及其相对速度，以模拟异构计算环境。
    5.  **数据封装**: 将生成的所有数据（邻接矩阵、成本矩阵/向量、处理器信息等）封装到一个标准化的`SchedulingProblem`数据类对象中，供调度环境使用。

---

## 3. 组件二：调度环境 (`environment.py`) —— 游戏世界

环境接收`ProblemGenerator`生成的实例，并将其作为“游戏地图”，提供与智能体交互的标准化接口。

*   **状态表示 (`S`)**: 状态被表示为一组固定尺寸的张量，通过填充（Padding）至预设的最大容量（`N_MAX`个任务，`M_MAX`个处理器）以适应神经网络的输入要求，并辅以掩码（Mask）来区分真实数据与填充数据。
    *   **静态张量 (问题定义)**: `adj_matrix`, `comp_costs`, `comm_costs`, `proc_speeds`, `task_mask`, `proc_mask`。
    *   **动态张量 (调度执行状态)**: `task_status` (One-Hot编码: `[未调度, 已就绪, 运行中, 已完成]`), `task_finish_times`, `processor_available_times`。

*   **动作空间 (`A`)**: 一个动作 `a = (task_id, proc_id)` 被定义为将任务 `i` 指派给处理器 `p`。
    *   **动作张量**: 动作空间被展平为一个尺寸为 `N_MAX * M_MAX` 的一维向量。
    *   **动作掩码**: 环境在每一步提供一个`legal_actions_mask` (布尔型, `N_MAX * M_MAX`)。一个动作`(i, p)`是合法的，当且仅当任务`i`的状态为“已就绪”。

*   **状态转移逻辑**: 当环境接收到一个合法的动作后，它将精确计算该任务的最早开始时间与完成时间，并据此更新所有相关的动态状态张量，包括被调度任务的状态、处理器的可用时间，以及任何因前驱任务完成而变为“已就绪”的新任务。

*   **高性能克隆 (`clone()`)**: 为了支持MCTS的高通量模拟，环境实现了一个**超轻量级的`clone`方法**。该方法通过将环境状态明确划分为**静态属性**（在单局游戏生命周期内不变，通过引用传递共享）和**动态属性**（在每个`step`中改变，进行深度复制），极大地降低了为MCTS模拟创建环境副本的开销。

*   **终止与奖励**:
    *   **终止条件**: 当所有真实任务的状态均为“已完成”时，一局（Episode）结束。
    *   **原始奖励 (`z_raw`)**: 奖励是最终完工时间的负值：`z_raw = -max(task_finish_times)`。

---

## 4. 组件三：双头图Transformer神经网络 (`model.py`) —— 认知核心

此网络是智能体的“大脑”，旨在通过深度学习逼近调度问题的策略函数和价值函数。为了赋予模型学习复杂长程依赖的能力，我们采用了强大的**图Transformer (Graph Transformer)** 架构。

*   **输入预处理**:
    *   **节点特征**: 构建一个 `N_MAX x D_node_features` 的特征矩阵。每一行代表一个任务节点的特征向量，由 **[任务状态的One-Hot编码, 归一化的计算成本]** 拼接而成。
    *   **节点嵌入**: 通过一个可学习的线性层 (`node_embedder`)，将初始节点特征映射到一个更高维的、模型内部的工作空间（`embed_dim`）。
    *   **位置编码**: 为每个节点添加一个独一无二、可学习的位置嵌入向量。由于Transformer本身是排列不变的，位置编码对于让模型区分不同任务（例如任务0和任务1）至关重要。

*   **图Transformer编码器**:
    *   **核心机制 (全局自注意力)**: 与仅在相邻节点间传递信息的传统GNN不同，图Transformer的核心是一个多头自注意力层。在该层中，**每个任务节点都可以直接与图中所有其他任务节点进行信息交互**。这创建了一个**全局感受野 (Global Receptive Field)**，使模型能够从第一层开始就捕捉长距离依赖关系，例如识别贯穿整个图的**全局关键路径**。
    *   **注入图结构信息 (注意力偏置)**: 为了让排列不变的Transformer理解DAG的拓扑结构和通信成本，我们将这些信息编码为一个**注意力偏置矩阵**。在计算任意两个节点`i`和`j`的注意力分数时，我们会加上一个偏置项。其构建逻辑如下：
        1.  对于图中**不存在直接依赖边**的节点对`(i, j)`，它们之间的注意力会受到一个巨大的负偏置，从而在softmax后概率趋近于零，被有效“屏蔽”。
        2.  对于图中**存在直接依赖边**的节点对`(i, j)`，它们的注意力偏置则由一个**可学习的函数**（通过`edge_encoder`线性层实现）根据其**归一化的通信成本**来决定。这允许模型在端到端的训练中自动学习如何根据通信开销来权衡节点间的重要性。

*   **输出头架构**:
    1.  **策略头 (`π_θ`)**:
        *   **输入**: 将图Transformer输出的每个任务的最终嵌入，与编码后的处理器特征（归一化速度、归一化可用时间）进行融合。
        *   **网络**: 一个多层感知机（MLP）。
        *   **输出**: 一个尺寸为 `N_MAX * M_MAX` 的**logits**向量，代表将每个任务分配给每个处理器的倾向性分数。通过`softmax`后生成策略概率分布`p`。
    2.  **价值头 (`v_θ`)**:
        *   **输入**: 对所有真实任务的最终嵌入进行带掩码的全局平均池化，得到一个代表整个图当前状态的单一向量`g`。
        *   **网络**: 一个独立的MLP。
        *   **输出**: 一个单一的标量值`v`，代表对当前状态的**归一化价值**（即最终奖励的Z-score）的预测。

*   **损失函数**:
    总损失是价值损失和策略损失的加权和，通过端到端的反向传播进行优化：
    `L(θ) = E[(z - v)² - πᵀ log(p) + c||θ||²]`
    其中 `z` 是MCTS生成的目标价值（经过归一化），`π` 是MCTS生成的目标策略。

---

## 5. 组件四：MCTS与自适应启发式引导 (`mcts.py`) —— 规划引擎

MCTS通过前瞻性搜索，将神经网络的战略性直觉转化为战术上更优的决策。本项目的MCTS实现经过了深度优化，集成了**批处理**和创新的**自适应启发式引导**机制。

*   **批处理MCTS (Batch MCTS)**: MCTS在选择阶段会收集多个待评估的叶子节点，然后将它们打包成一个批次，通过一次神经网络前向传播进行统一评估。这种方法极大地提升了GPU的利用率和搜索吞吐量。

*   **自适应启发式引导**: 为了在训练初期加速学习，MCTS被一个经典的启发式算法（HEFT）所引导。
    *   **状态化HEFT调度器**: 我们实现了一个`HEFTStatefulScheduler`。它在游戏开始时一次性预计算所有任务的优先级（向上排名）。在MCTS搜索的任何节点，都可以查询它以获取当前状态下“HEFT推荐”的最佳动作。
    *   **软性概率引导**: 我们不强制MCTS遵循启发式建议，而是采用“软性”引导。在PUCT公式中使用的先验概率是一个**混合概率分布**：`p_hybrid = (1 - ε) * p_nn + ε * p_heft`，其中`p_nn`是网络策略，`p_heft`是HEFT动作的独热向量，`ε`是引导强度。
    *   **置信度门控 (Confidence-Based Gating)**: (高级) 如果网络自身对某个**非HEFT动作**的预测已经**极度自信**（概率超过阈值），则本次搜索会临时减小引导强度`ε`，允许智能体优先相信自己学到的、可能超越启发式的策略。

*   **探索与策略生成**:
    *   **狄利克雷噪声**: 在每次MCTS搜索开始前，对根节点的先验策略叠加狄利克雷噪声，以强制MCTS探索神经网络认为可能性较低的初始路径，这是发现新策略的关键。
    *   **温度采样**: 在自对弈的前期步骤中，根据带温度的访问次数分布进行**随机抽样**来选择动作，以保证轨迹的多样性。在游戏后期，则贪婪地选择访问次数最多的动作。
    *   **最终策略 (`π`)**: 根节点各子节点访问次数的归一化分布，将作为训练神经网络策略头的改进标签。

---

## 6. 组件五：训练管线 —— 自我进化的生态系统

这是一个集成了所有组件的、闭环的、自动化的系统，负责协调智能体的学习与进化。

1.  **阶段零：专家知识预训练 (`expert_data_generator.py`)**:
    *   **目标**: 克服“冷启动”问题，为网络提供一个具备强大初始策略和价值判断能力的起点。
    *   **流程**: 离线生成一个包含数万问题解的专家数据集。在该数据集中，HEFT调度器求解每个问题，其完整的决策轨迹 `(状态, 动作, 最终回报)` 被记录下来。然后，使用这个数据集对网络的**策略头和价值头同时进行监督学习预训练**。该预训练模型将作为初始的`best_model`。

2.  **阶段一：自对弈 (`main.py`)**:
    *   **流程**: 多个工作进程使用当前的`best_model`并行地进行自对弈。每个工作进程内部都使用带引导的MCTS来决策。
    *   **产出**: 每局游戏的历史记录，包含 `(状态, MCTS改进策略, 最终归一化回报)` 元组，被存入一个全局的`ReplayBuffer`。

3.  **奖励信号处理：在线归一化 (`trainer.py`)**:
    *   **问题**: 原始奖励（负完工时间）的尺度随问题复杂度剧烈变化，会严重干扰价值网络的稳定训练。
    *   **方案**: `Trainer`模块中实现了一个**在线奖励归一化**机制。它使用**Welford在线算法**实时维护所有已完成游戏原始奖励的全局运行均值`μ`和标准差`σ`。
    *   **应用**: 一局游戏结束后，其原始奖励`z_raw`会被转换为一个尺度稳定的归一化价值目标`z = (z_raw - μ) / σ`。这个`z`（一个Z-score）将被用作价值头的监督信号。

4.  **阶段二：训练 (`trainer.py`)**:
    *   **流程**: `Trainer`持续从`ReplayBuffer`中随机采样小批量经验数据，计算损失并更新网络权重，生成一个`candidate_model`。在支持的GPU上，此阶段会启用**自动混合精度（AMP）**训练以提升速度和效率。

5.  **阶段三：竞技场：评估与晋升 (`trainer.py`)**:
    *   **触发**: 训练器会定期将`candidate_model`提交至竞技场。
    *   **协议**: `candidate_model`与当前的`best_model`在一个固定的、大规模的评估数据集上进行多场“对战”。该评估过程为最大化效率也进行了**完全并行化**。
    *   **晋升**: 如果`candidate_model`的胜率超过预设阈值（例如**55%**），它将被晋升为新的`best_model`。只有晋升成功的模型才会被部署用于下一轮的自对弈数据生成。

6.  **阶段四：自适应引导退火 (`main.py`)**:
    *   **目标**: 随着智能体能力的提升，平滑地移除外部引导，使其最终实现完全自主决策。
    *   **机制**: 引导强度`ε`在每次评估后，会根据模型相较于HEFT基准的平均性能改善率进行动态衰减。
    *   **风险控制**: 系统内置了安全网。如果一个新晋升的模型表现出显著的性能衰退，系统可以临时**重新增强引导**，即适度增加`ε`，以帮助智能体稳定并恢复性能。

---

## 7. 未来优化方向

为进一步将系统性能推向理论最优，以下是当前代码库**尚未实现**，但极具潜力的未来探索方向：

*   **优先经验回放 (Prioritized Experience Replay, PER)**: 当前的回放池是均匀采样的。采用PER可以根据TD误差`|z - v|`的大小，优先采样那些网络预测不准的、“信息量大”的经验，从而加速学习过程。
*   **状态与评估缓存 (Transposition Table)**: 在单次MCTS搜索中，不同的动作序列可能导致相同的环境状态。通过哈希表缓存已访问过的状态及其神经网络评估结果，可以避免对同一状态的重复计算，将更多的模拟次数投入到未知的状态空间中。
*   **系统化超参数优化 (Systematic Hyperparameter Optimization)**: 当前超参数是在`config.py`中手动设置的。采用如Optuna, Ray Tune等自动化调优框架，可以对学习率、`c_puct`、网络结构等关键超参数进行系统化的搜索，以找到最优组合。