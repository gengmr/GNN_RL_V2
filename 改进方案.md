提案：监督式预训练阶段的战略优化
1. 概要
基于对当前训练动态的分析，即策略与价值双重损失函数 (P_loss + V_loss) 表现出不稳定的收敛模式，现提议对预训练阶段进行战略性重构。此方案将预训练的目标从一个多任务学习框架，精简为一个单一的、高保真度的行为克隆 (Behavioral Cloning) 目标。
核心调整为在监督学习阶段完全暂停价值函数 (V_loss) 的训练。训练过程将唯一地聚焦于优化策略网络，使其能够精确复现HEFT专家调度器的分步决策逻辑（仅P_loss）。此举旨在建立一个稳定且可预测的预训练基线，产出一个具备可靠初始策略的模型，为后续的强化学习阶段提供一个更坚实的出发点。
2. 战略变更的理论依据
当前的 V_loss 目标函数，基于一个跨问题实例进行全局归一化的完工时间 (-heft_makespan)，被识别为训练不稳定性的一个潜在驱动因素。该目标函数存在以下内在问题：
目标信号的不一致性: 该目标迫使模型对内在难度与规模完全不同的问题实例进行价值比较，这在理论上缺乏一致性，从而引入了噪声到学习信号中。
损失曲面的复杂性: 一个存在内在矛盾的目标函数可能会导致一个高度复杂和非凸的损失曲面，使得优化过程倾向于在局部区域震荡，而非向全局最优解平稳收敛。
通过将训练目标隔离为单一的 P_loss，我们将学习任务约束在一个目标明确、逻辑一致且理论上更为健全的框架内：即直接模仿专家的示范行为。
3. 按操作领域的修改方案
此战略转移将对三个核心操作领域产生影响：专家数据生成、数据处理流程，以及模型训练逻辑。
3.1. 领域：专家数据生成
目标: 优化数据生成流程，仅产出策略模仿任务所必需的数据。
操作变更: 调整数据生成脚本 (expert_data_generator.py)，使其停止计算和存储estimated_return（该值由-heft_makespan派生）。
影响分析:
数据结构变更: 生成的专家轨迹数据结构将从 (状态, 策略, 价值) 元组简化为 (状态, 策略) 元组。
计算效率: 该变更将消除为每个问题实例运行完整HEFT调度算法以获取最终完工时间的步骤，从而可能提升数据生成的整体吞吐量。
存储优化: 输出的数据集文件 (.pkl) 将因数据结构的简化而减小体积，提高存储和I/O效率。
3.2. 领域：数据加载与处理
目标: 使数据处理流水线适配新的、不含价值信息的数据格式。
操作变更:
更新 ExpertDataset 类定义，以准确反映其所封装的数据为 (状态, 策略) 元组。
为预训练阶段的 DataLoader 配置一个专用的数据整理函数 (collate_fn)。此函数的功能是正确地将 (状态, 策略) 元组列表批量处理为 (批量状态张量, 批量策略张量)。原有的 collate_fn 将被保留，以服务于后续强化学习阶段对 (状态, 策略, 价值) 数据的需求。
影响分析: 此项变更确保了不同训练阶段之间数据处理逻辑的隔离，避免了因数据格式不匹配可能引发的运行时错误，增强了代码的鲁棒性。
3.3. 领域：模型训练逻辑 (预训练阶段)
目标: 将梯度更新的来源完全限制在策略网络。
操作变更:
前向传播: 在每个训练步骤中，模型仍执行完整的前向传播，同时计算策略 logits 和价值预测。然而，价值预测的输出将被逻辑上忽略，不参与任何后续计算。
损失计算: 预训练步骤的总损失 (total_loss) 将被严格定义为策略损失 (P_loss)，即交叉熵损失。价值损失 (V_loss) 的计算将被完全跳过。
反向传播: 优化器所依据的梯度将仅通过 P_loss 进行反向传播。因此，学习信号将不会更新价值头的专属权重，而共享的骨干网络（GNN层）的权重更新将仅由其对策略预测的贡献度决定。
日志与监控: 所有与V_loss相关的监控指标、进度条显示和日志记录将被移除或调整，以准确反映训练过程对P_loss的单一关注。
4. 预期后果与影响分析
模型状态: 预训练完成后，模型的策略头将被充分训练以模仿专家行为，而价值头将保持其随机初始化的权重状态。这是该战略的预期结果。
强化学习阶段的初始化: 强化学习阶段将从一个未经训练的价值函数开始，这构成了价值评估上的“冷启动”。MCTS在初始探索阶段将依赖一个无信息的价值信号，可能会降低早期自我对弈的样本效率。然而，一个经过良好预训练的策略网络能够提供高质量的初始动作选择，引导智能体产生有意义的轨迹，从而为价值函数在强化学习初期的快速学习创造有利条件。
训练稳定性: 预期预训练过程的稳定性将得到显著改善，P_loss 将展现出更清晰的收敛趋势，为模型性能评估提供更可靠的指标。